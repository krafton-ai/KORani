{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "user_template=\"\"\"[Question]\n",
    "Summarize the following meeting note into 3 Korean sentences\n",
    "{Summary}\n",
    "[The Start of Assistant 1's Answer]\n",
    "{Answer1}\n",
    "[The End of Assistant 1's Answer]\n",
    "[The Start of Assistant 2's Answer]\n",
    "{Answer2}\n",
    "[The End of Assistant 2's Answer]\n",
    "[System]\n",
    "We would like to request your feedback on the performance of two AI assistants in response to the user question displayed above.\n",
    "Please rate the clarity, completeness, and brevity of their responses. Each assistant receives an overall score on a scale of 0 to 10, where a higher score indicates better overall performance. \n",
    "The detailed guideline for the scoring is the following:\n",
    "Coverage (3 points): A high-quality summarization system should be able to identify and extract key points, main arguments, and critical details.\n",
    "Coherence (2.5 points): The generated summary should be logically organized and easy to understand, with a coherent structure, well-formed sentences, and smooth transitions between ideas\n",
    "Conciseness (2.5 points): The summary should be concise, removing redundancy and unnecessary information while still conveying the main points effectively.\n",
    "Accuracy (2 points): The generated summary should maintain the accuracy of the original content and not introduce errors or misrepresentations.\n",
    "Let's say the score of Assistant 1 is 2, 2, 2, 2 for Coverage, Coherence, Conciseness, and Accuracy, respectively.\n",
    "Please first provide detailed explanations, specifying at which part of the example you think it is, avoiding any potential bias, and ensuring that the order in which the responses were presented does not affect your judgment. \n",
    "Please provide one criteria per each paragraph and give the score to assistant 2 compared to the score of Assistant 1.  \n",
    "In the subsequent line, please output a single line containing only two values indicating the scores for Assistant 1 and 2, respectively. The two scores are separated by whitespace. \n",
    "You may speak in English.\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "assist_template = \"\"\"For Coverage, {At which part does Assistant 2 cover more/less than Assistant 1 in 3 sentences}.\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('summary.txt', 'r') as file:\n",
    "    Summary = file.read()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "metrics = [\"Coverage\", \"Coherence\", \"Conciseness\", \"Accuracy\"]\n",
    "summ_prompt = []\n",
    "Answer1 = \"\"\" \"\"\"\n",
    "Answer2 = \"\"\" \"\"\"\n",
    "for k in range(4):\n",
    "    summ_prompt.append(f\"\"\"[Question]\n",
    "Summarize the following meeting note into 3 English sentences\n",
    "{Summary}\n",
    "[The Start of Assistant 1's Answer]\n",
    "{Answer1}\n",
    "[The End of Assistant 1's Answer]\n",
    "[The Start of Assistant 2's Answer]\n",
    "{Answer2}\n",
    "[The End of Assistant 2's Answer]\n",
    "[System]\n",
    "We would like to request your feedback on the performance of two AI assistants in response to the user question displayed above.\n",
    "Each assistant receives an overall score on a scale of 0 to 10, where a higher score indicates better overall performance. \n",
    "The detailed guideline for the scoring is the following:\n",
    "Coverage (3 points): A high-quality summarization system should be able to identify and extract key points, main arguments, and critical details.\n",
    "Coherence (2.5 points): The generated summary should be logically organized and easy to understand, with a coherent structure, well-formed sentences, and smooth transitions between ideas\n",
    "Conciseness (2.5 points): The summary should be concise, removing redundancy and unnecessary information while still conveying the main points effectively.\n",
    "Accuracy (2 points): The generated summary should maintain the accuracy of the original content and not introduce errors or misrepresentations.\n",
    "Let's say the score of Assistant 1 is 2, 2, 2, 2 for Coverage, Coherence, Conciseness, and Accuracy, respectively.\n",
    "Please first provide detailed explanations, specifying at which part of the example you think it is, avoiding any potential bias, and ensuring that the order in which the responses were presented does not affect your judgment. \n",
    "Please provide the detailed explanations specifying at which part of the example is better or worse and give the score to assistant 2 compared to the score of Assistant 1.  \n",
    "First, Let's start with the {metrics[k]} score. You may speak in English.\"\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import openai\n",
    "import requests, json\n",
    "\n",
    "headers = {\n",
    "    \"Content-Type\": \"application/json\",\n",
    "    \"Authorization\": f\"Bearer {openai.api_key}\",\n",
    "    \"OpenAI-Organization\": openai.organization\n",
    "}\n",
    "\n",
    "import torch\n",
    "import numpy as np\n",
    "\n",
    "from transformers import AutoTokenizer, DPRQuestionEncoder, DPRContextEncoder\n",
    "from typing import List\n",
    "\n",
    "output = \"\"\n",
    "for k in range(4):\n",
    "    message = [{\"role\":\"user\", \"content\":user_template}, \n",
    "    {\"role\":\"assistant\", \"content\":assist_template}, \n",
    "    {\"role\":\"user\", \"content\":summ_prompt[k]}]\n",
    "    data = {\n",
    "        \"model\":\"gpt-4\",\n",
    "        \"messages\" : message,\n",
    "        \"temperature\" : 0.1,\n",
    "        \"max_tokens\" : 512,\n",
    "        \"frequency_penalty\" : 0.2,\n",
    "        \"stop\": [\"\\n\\n\", \"\\nFor\"]\n",
    "    }\n",
    "    response = None\n",
    "    while True:\n",
    "        try:\n",
    "            response = requests.post(\n",
    "                \"https://api.openai.com/v1/chat/completions\",\n",
    "                headers=headers,\n",
    "                data=json.dumps(data),\n",
    "                timeout=60,\n",
    "            )\n",
    "            break\n",
    "        except requests.exceptions.Timeout:\n",
    "            print(f\"Request timed out after 15\")\n",
    "    output += response.json()[\"choices\"][0][\"message\"][\"content\"]\n",
    "    print(response.json()[\"choices\"][0][\"message\"][\"content\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "user_template = \"\"\"[Question]\n",
    "Translate the following sentences into Korean.\n",
    "{Sentences}\n",
    "[The Start of Assistant 1's Answer]\n",
    "{Answer1}\n",
    "[The End of Assistant 1's Answer]\n",
    "[The Start of Assistant 2's Answer]\n",
    "{Answer2}\n",
    "[The End of Assistant 2's Answer]\n",
    "[System]\n",
    "We would like to request your feedback on the performance of two AI assistants in response to the user question displayed above.\n",
    "Please rate the clarity, completeness, and brevity of their responses. Each assistant receives an overall score on a scale of 0 to 10, where a higher score indicates better overall performance. \n",
    "The detailed guideline for the scoring is the following:\n",
    "Accuracy (4 points): Ensuring that the translation accurately conveys the meaning and intent of the source text is crucial for effective communication.\n",
    "Fluency (3 points): A fluent, natural-sounding translation makes the text more readable and accessible to the target audience.\n",
    "Cultural Appropriateness (3 points): Adapting the translation to the target culture ensures that it resonates with the audience and avoids any potential misunderstandings.\n",
    "Let's say the score of Assistant 1 is 3, 2, 2 for Accuracy, Fluency, and Cultural Appropriateness, respectively.\n",
    "Please first provide detailed explanations, specifying at which part of the example you think it is, avoiding any potential bias, and ensuring that the order in which the responses were presented does not affect your judgment. \n",
    "Please provide one criteria per each paragraph and give the score to assistant 2 compared to the score of Assistant 1.  \n",
    "You may speak in English.\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "assist_template = \"\"\"For Accuracy, {At which part is Assistant 2 more/less accurate than Assistant 1 in 3 sentences}.\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "metrics = [\"Accuracy\", \"Fluency\", \"Cultural Appropriateness\"]\n",
    "summ_prompt = []\n",
    "Sentences = \"\"\" \"\"\"\n",
    "Answer1 = \"\"\" \"\"\"\n",
    "Answer2 = \"\"\" \"\"\"\n",
    "for k in range(len(metrics)):\n",
    "    summ_prompt.append(f\"\"\"[Question]\n",
    "Translate the following sentences into English.\n",
    "{Sentences}\n",
    "[The Start of Assistant 1's Answer]\n",
    "{Answer1}\n",
    "[The End of Assistant 1's Answer]\n",
    "[The Start of Assistant 2's Answer]\n",
    "{Answer2}\n",
    "[The End of Assistant 2's Answer]\n",
    "[System]\n",
    "We would like to request your feedback on the performance of two AI assistants in response to the user question displayed above.\n",
    "Please rate the clarity, completeness, and brevity of their responses. Each assistant receives an overall score on a scale of 0 to 10, where a higher score indicates better overall performance. \n",
    "The detailed guideline for the scoring is the following:\n",
    "Accuracy (4 points): Ensuring that the translation accurately conveys the meaning and intent of the source text is crucial for effective communication.\n",
    "Fluency (3 points): A fluent, natural-sounding translation makes the text more readable and accessible to the target audience.\n",
    "Cultural Appropriateness (3 points): Adapting the translation to the target culture ensures that it resonates with the audience and avoids any potential misunderstandings.\n",
    "Let's say the score of Assistant 1 is 3, 2, 2 for Accuracy, Fluency, and Cultural Appropriateness, respectively.\n",
    "Please first provide detailed explanations, specifying at which part of the example you think it is, avoiding any potential bias, and ensuring that the order in which the responses were presented does not affect your judgment. \n",
    "Please provide the detailed explanations specifying at which part of the example is better or worse and give the score to assistant 2 compared to the score of Assistant 1.  \n",
    "First, Let's start with the {metrics[k]} score. You may speak in English.\"\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import openai\n",
    "import requests, json\n",
    "\n",
    "headers = {\n",
    "    \"Content-Type\": \"application/json\",\n",
    "    \"Authorization\": f\"Bearer {openai.api_key}\",\n",
    "    \"OpenAI-Organization\": openai.organization\n",
    "}\n",
    "\n",
    "import torch\n",
    "import numpy as np\n",
    "\n",
    "from transformers import AutoTokenizer, DPRQuestionEncoder, DPRContextEncoder\n",
    "from typing import List\n",
    "\n",
    "output = \"\"\n",
    "for k in range(len(metrics)):\n",
    "    message = [{\"role\":\"user\", \"content\":user_template}, \n",
    "    {\"role\":\"assistant\", \"content\":assist_template}, \n",
    "    {\"role\":\"user\", \"content\":summ_prompt[k]}]\n",
    "    data = {\n",
    "        \"model\":\"gpt-4\",\n",
    "        \"messages\" : message,\n",
    "        \"temperature\" : 0.1,\n",
    "        \"max_tokens\" : 512,\n",
    "        \"frequency_penalty\" : 0.2,\n",
    "        \"stop\": [\"\\n\\n\"]\n",
    "    }\n",
    "    response = None\n",
    "    while True:\n",
    "        try:\n",
    "            response = requests.post(\n",
    "                \"https://api.openai.com/v1/chat/completions\",\n",
    "                headers=headers,\n",
    "                data=json.dumps(data),\n",
    "                timeout=60,\n",
    "            )\n",
    "            break\n",
    "        except requests.exceptions.Timeout:\n",
    "            print(f\"Request timed out after 15\")\n",
    "    output += response.json()[\"choices\"][0][\"message\"][\"content\"]\n",
    "    print(response.json()[\"choices\"][0][\"message\"][\"content\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Assistant 2's total score is calculated as follows:\n",
      "\n",
      "Coverage: 1.5\n",
      "Coherence: 1.5\n",
      "Conciseness: 1.5\n",
      "Accuracy: 1\n",
      "\n",
      "Total Score: 1.5 + 1.5 + 1.5 + 1 = 5.5\n"
     ]
    }
   ],
   "source": [
    "message = [{\"role\":\"user\", \"content\":output + \"\\nSo in total, What's the total score of Assistant 2?\"}]\n",
    "data = {\n",
    "    \"model\":\"gpt-4\",\n",
    "    \"messages\" : message,\n",
    "    \"temperature\" : 0.1,\n",
    "    \"max_tokens\" : 512,\n",
    "    \"frequency_penalty\" : 0.2\n",
    "}\n",
    "response = None\n",
    "while True:\n",
    "    try:\n",
    "        response = requests.post(\n",
    "            \"https://api.openai.com/v1/chat/completions\",\n",
    "            headers=headers,\n",
    "            data=json.dumps(data),\n",
    "            # timeout=15,\n",
    "        )\n",
    "        break\n",
    "    except requests.exceptions.Timeout:\n",
    "        print(f\"Request timed out after 15\")\n",
    "print(response.json()[\"choices\"][0][\"message\"][\"content\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "py38",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.16 | packaged by conda-forge | (default, Feb  1 2023, 16:01:55) \n[GCC 11.3.0]"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "bed9faf9e7692d4f90c21d66e1b265c4c1465a3bba4d8ec4dc376bebf7e21a8b"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
