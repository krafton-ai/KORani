# KORani

- KORani: Large Language Models for ğŸ‡°ğŸ‡· Korean and ğŸ‡ºğŸ‡¸ English using LLaMA 13B and Polyglot 12.8B.
- Tested which LLM is effective for ğŸ‡°ğŸ‡· Korean tasks after finetuning.
- ğŸ¤— You can download the weights from the [Link](https://huggingface.co/KRAFTON).

## Release
<p align="center">
<a href=""><img src="assets/KORani.png" width="20%"></a>
</p>
 
This repository contains inference code for KORani models that are based on [LLaMA 13B](https://arxiv.org/abs/2302.13971v1) and [Polyglot 12.8B](https://huggingface.co/EleutherAI/polyglot-ko-12.8b).
KORani models are finetuned using [ShareGPT](https://huggingface.co/datasets/anon8231489123/ShareGPT_Vicuna_unfiltered/tree/main) & [KoVicuna](https://huggingface.co/datasets/junelee/sharegpt_deepl_ko) dataset. This work is hugely influenced by [Vicuna](https://github.com/lm-sys/FastChat) project.

### Models
We offer three types of models as follows.


| Model | Base | Train dataset | Huggingface Link |
| --- | ---: | ---: | ---: |
| 1ï¸âƒ£ KORani-v1-13B | Polyglot 12.8B | KoVicuna dataset | [Link 1](https://huggingface.co/KRAFTON/KORani-v1-13B) |
| 2ï¸âƒ£ KORani-v2-13B | LLaMA 13B | KoVicuna dataset | [Link 2](https://huggingface.co/KRAFTON/KORani-v2-13B) |
| 3ï¸âƒ£ KORani-v3-13B | LLaMA 13B | ShareGPT & KoVicuna dataset | [Link 3](https://huggingface.co/KRAFTON/KORani-v3-13B) |


### Notes
* We used LLaMA 13B from [here](https://huggingface.co/decapoda-research/llama-13b-hf).
* We extracted only the data from [Kovicuna](https://huggingface.co/datasets/junelee/sharegpt_deepl_ko) that corresponds to the first and second parts of the conversation, which are 'human' and 'GPT'.
* The model finetuning was conducted on eight A100 40GB GPUs. The code used for training is based on the [Fastchat](https://github.com/lm-sys/FastChat).

## Local Setup

1. Install dependencies
   ```bash
   pip install -r requirements.txt
   ```

## Inference
### Command <br>
    `--model_path` (str): model path for evaluation. (e.g. KRAFTON/KORani-v3-13B) <br>
    `--task` (str): choose which task you want to evaluate. (e.g. only [QA, summarization, translation] are available in this repo.) <br>


### Question Answering

    ```bash
    python inference.py \
        --model_path "KRAFTON/KORani-v3-13B"
        --task "QA"
    ```

    Below is the prompt used to generate the answer. You can modify it in the [QA link](prompts/QA.txt).

    ```python
    PROMPT = """\
    ìš°ë¦¬ëŠ” ì•„ë˜ì™€ ê°™ì€ ì •ë³´ë¥¼ ê°–ê³  ìˆìŠµë‹ˆë‹¤.
    ---------------------
    {context}
    ---------------------
    ### ì£¼ì–´ì§„ ì •ë³´ì— ë”°ë¼, ì§ˆë¬¸ì— ë‹µí•´ì£¼ì„¸ìš”.: '{question}'
    ### Assistant:"""
    ```

### QA sample result
```python
context = ""
question = ""
```

| Model | Target |
| --- | ---: |
| KORani-v1-13B |  |
| KORani-v2-13B |  |
| KORani-v3-13B |  |
| Vicuna 13B |  |
| Koalpaca-13B |  |
| ChatGPT 3.5 |  |
| GPT4 |  |

### Translation

    ```bash
    python inference.py \
        --model_path "KRAFTON/KORani-v3-13B"
        --task "translation"
    ```

    Below is the prompt used to generate the answer. You can modify it in the [translation link](prompts/translation.txt). If you use few-shot in translation, the performance can improve.

    ```python
    PROMPT = """\
    ### Instruction: Translate English sentence into Korean. You may leave specific names as they are.
    English: ì–¼ë§ˆë‚˜ ë§ì€ ì–¸ì–´ë¥¼ ë§í•  ìˆ˜ ìˆë‹ˆ?
    Korean: How many languages can you speak?#
    English: ì¼ ë‹¤ ëë‚¬ì–´?
    Korean: Did you finish your work?#
    English: {source_sentence} 
    Korean:"""
    ```

### Translation sample result
```python
source_sentence = "Hey! I have some Kissflow Legal Review requests that are blocked by Tax and Accounting, can this be looked at?"
```

| Model | Target |
| --- | ---: |
| KORani-v1-13B | "ì•ˆë…•í•˜ì„¸ìš”! ì„¸ê¸ˆê³¼ íšŒê³„ë¡œ ì¸í•´ ë§‰í˜€ ìˆëŠ” í‚¤ìŠ¤í”Œë¡œìš° ë²•ë¥  ê²€í†  ìš”ì²­ì´ ëª‡ ê°œ ìˆëŠ”ë°, ê²€í† í•´ ì£¼ì‹¤ ìˆ˜ ìˆë‚˜ìš”?" |
| KORani-v2-13B | "ì•ˆë…•í•˜ì„¸ìš”! ì œê°€ Kissflow Legal Review ìš”ì²­ì„ ëª©ê²©í–ˆëŠ”ë°, ì„¸ë¬´ ë° íšŒê³„ ë¶€ì„œì—ì„œ ì°¨ë‹¨í•˜ê³  ìˆëŠ”ë° ì´ ë¬¸ì œë¥¼ ì¡°ì‚¬í•´ ì£¼ì‹œê² ì–´ìš”?" |
| KORani-v3-13B | "ì•ˆë…•í•˜ì„¸ìš”! ì €ëŠ” Kissflow Legal Review ìš”ì²­ì´ ì„¸ê¸ˆê³¼ íšŒê³„ì— ì˜í•´ ì°¨ë‹¨ë˜ê³  ìˆëŠ”ë°, ì´ ë¬¸ì œê°€ ì‚´í´ë³¼ ìˆ˜ ìˆì„ê¹Œìš”?" |
| Vicuna 13B | "ì•ˆë…•í•˜ì„¸ìš”! ì„¸ê¸ˆê³„ì‚°ê³¼ íšŒê³„ë¶€ì„œê°€ ì°¨ë‹¨í•´ ìˆëŠ” Kissflow Legal Review ìš”ì²­ì´ ëª‡ ê°œê°€ ìˆìŠµë‹ˆê¹Œ? ì´ê²ƒì„ ì‚´í´ë³´ì‹œê² ìŠµë‹ˆê¹Œ?" |
| Koalpaca-13B | "ì•¼! ì„¸ê¸ˆê³¼ íšŒê³„ì— ë§‰íŒ í‚¤ì‹±í”Œë¡œìš° ë²•ì  ê²€í†  ìš”ì²­ì´ ìˆëŠ”ë°, ì´ê±° ê²€í† í•´ì¤„ ìˆ˜ ìˆì–´?" |
| DeepL | "ì•ˆë…•í•˜ì„¸ìš”! ì„¸ë¬´ ë° íšŒê³„ì—ì„œ ì°¨ë‹¨ëœ Kissflow ë²•ë¥  ê²€í†  ìš”ì²­ì´ ëª‡ ê°œ ìˆëŠ”ë°ìš”, ì´ ìš”ì²­ì„ ì‚´í´ë³¼ ìˆ˜ ìˆë‚˜ìš”?" |
| ChatGPT 3.5 | "ì•ˆë…•í•˜ì„¸ìš”! ì„¸ë¬´ ë° íšŒê³„ ë¶€ì„œì—ì„œ ì°¨ë‹¨ëœ ëª‡ ê°€ì§€ Kissflow Legal Review ìš”ì²­ì´ ìˆìŠµë‹ˆë‹¤. í™•ì¸í•´ ì£¼ì‹¤ ìˆ˜ ìˆë‚˜ìš”?" |
| GPT4 | "í—¤ì´! ì œê°€ Tax and Accountingì— ì˜í•´ ì°¨ë‹¨ëœ ëª‡ ê°€ì§€ Kissflow ë²•ë¥  ê²€í†  ìš”ì²­ì´ ìˆëŠ”ë°, ì´ê²ƒì„ í™•ì¸í•´ ì£¼ì‹¤ ìˆ˜ ìˆë‚˜ìš”?" |


### Summarization

    ```bash
    python inference.py \
        --model_path "KRAFTON/KORani-v3-13B"
        --task "summarization"
    ```

    Below is the prompt used to generate the answer. You can modify it in the [summarization link](prompts/summarization.txt). It does not work for a max length of over 2048.

    ```python
    PROMPT = """\
    # Meeting note
    {target_document}

    # Summarize the meeting note into 3 Korean sentences.
    ### Output: 1)"""
    ```

### Summarization sample result

```python
target_document = ""
```

| Model | Target |
| --- | ---: |
| KORani-v1-13B |  |
| KORani-v2-13B |  |
| KORani-v3-13B |  |
| Vicuna 13B |  |
| Koalpaca-13B |  |
| ChatGPT 3.5 |  |
| GPT4 |  |

## Evaluation
We tested model performance using GPT-4, and the code and results of the test can be found through the [AutoEvalGPT](https://github.com/krafton-ai/AutoEvalGPT).

## Limitations
The Korean performance of our models is not as good as the English performance of [Vicuna](https://github.com/lm-sys/FastChat). We believe this is due to the not enough quality of foundation models in the Korean tasks (compared to Llama in English tasks) and the dataset quality, which is primarily translational. We will continue to update the new versions of the Korani models as soon as we achieve better results.

## License
Our github repo and models are intended for research purpose, non-commercial use only, subject to the model License of LLaMA, [Terms of Use](https://openai.com/policies/terms-of-use) of the data generated by OpenAI, and [Privacy Practices](https://chrome.google.com/webstore/detail/sharegpt-share-your-chatg/daiacboceoaocpibfodeljbdfacokfjb) of ShareGPT. Please contact us If you find any potential violation.
The code is released under the Apache License 2.0.
