# KORani

- KORani: Large Language Models for Korean and English using LLaMA 13B and Polyglot 12.8B.
- Tested which LLM is effective for Korean tasks after finetuning.
- You can download the models from the [Link](https://huggingface.co/KRAFTON).

### Release
<p align="center">
<a href=""><img src="assets/KORani.png" width="20%"></a>
</p>
 
This repository contains inference code for KORani models that are based on [LLaMA 13B](https://arxiv.org/abs/2302.13971v1) and [Polyglot 12.8B](https://huggingface.co/EleutherAI/polyglot-ko-12.8b).
KORani models are finetuned using [ShareGPT](https://huggingface.co/datasets/anon8231489123/ShareGPT_Vicuna_unfiltered/tree/main) & [KoVicuna](https://github.com/melodysdreamj/KoVicuna).
We offer three types of models as follows.
- KORani-v1-13B: finetuned using KoVicuna dataset based on Polyglot 12.8B <https://huggingface.co/KRAFTON/KORani-v1-13B>
- KORani-v2-13B: finetuned using KoVicuna dataset based on LLaMA 13B <https://huggingface.co/KRAFTON/KORani-v2-13B>
- KORani-v3-13B: finetuned using ShareGPT & KoVicuna datset based on LLaMA 13B <https://huggingface.co/KRAFTON/KORani-v3-13B>

* We used LLaMA 13B from [here](https://huggingface.co/decapoda-research/llama-13b-hf).
* The model training was conducted on eight A100 40GB GPUs. The code used for training is based on the [Link](https://github.com/lm-sys/FastChat).

### Local Setup

1. Install dependencies
   ```bash
   pip install -r requirements.txt
   ```

### Inference

1. Question Answering

```bash
python inference.py \
    --model_path "KRAFTON/KORani-v3-13B"
    --task "QA"
```

Below is the prompt used to generate the answer. You can modify it in the [link](prompts/QA.txt).

```python
PROMPT = """\
우리는 아래와 같은 정보를 갖고 있습니다.
---------------------
{context}
---------------------
### 주어진 정보에 따라, 질문에 답해주세요.: '{question}'
### Assistant:"""
```

2. Translation

```bash
python inference.py \
    --model_path "KRAFTON/KORani-v3-13B"
    --task "translation"
```

Below is the prompt used to generate the answer. You can modify it in the [link](prompts/translation.txt). If you use few-shot in translation, the performance can improve.

```python
PROMPT = """\
### Instruction: Translate Korean sentence into English. You may leave specific names as they are.
Korean: 얼마나 많은 언어를 말할 수 있니?
English: How many languages can you speak?#
Korean: 일 다 끝났어?
English: Did you finish your work?#
Korean: {Target_sentence} 
English:"""
```

3. Summarization

```bash
python inference.py \
    --model_path "KRAFTON/KORani-v3-13B"
    --task "summarization"
```

Below is the prompt used to generate the answer. You can modify it in the [link](prompts/summarization.txt). It does not work for a max length of over 2048.

```python
PROMPT = """\
# Meeting note
{Target_document}

# Summarize the meeting note into 3 Korean sentences.
### Output: 1)"""
```

### License
Our github repo and models are intended for research purpose, non-commercial use only, subject to the model License of LLaMA, [Terms of Use](https://openai.com/policies/terms-of-use) of the data generated by OpenAI, and [Privacy Practices](https://chrome.google.com/webstore/detail/sharegpt-share-your-chatg/daiacboceoaocpibfodeljbdfacokfjb) of ShareGPT. Please contact us If you find any potential violation.
The code is released under the Apache License 2.0.

### Contact
Gibbeum Lee (pirensisco@krafton.com)
Seongjun Yang (seongjunyang@krafton.com)