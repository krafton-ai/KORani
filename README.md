# KORani

- KORani: Large Language Models for ğŸ‡°ğŸ‡· Korean and ğŸ‡ºğŸ‡¸ English using LLaMA 13B and Polyglot 12.8B.
- Tested which LLM is effective for ğŸ‡°ğŸ‡· Korean tasks after finetuning.
- ğŸ¤— You can download the weights from the [Link](https://huggingface.co/KRAFTON).

## Release
<p align="center">
<a href=""><img src="assets/KORani.png" width="20%"></a>
</p>
 
This repository contains inference code for KORani models that are based on [LLaMA 13B](https://arxiv.org/abs/2302.13971v1) and [Polyglot 12.8B](https://huggingface.co/EleutherAI/polyglot-ko-12.8b).
KORani models are finetuned using [ShareGPT](https://huggingface.co/datasets/anon8231489123/ShareGPT_Vicuna_unfiltered/tree/main) & [KoVicuna](https://huggingface.co/datasets/junelee/sharegpt_deepl_ko) dataset. This work is hugely influenced by [Vicuna](https://github.com/lm-sys/FastChat) project.

### Models
We offer three types of models as follows.
- 1ï¸âƒ£ KORani-v1-13B: Polyglot 12.8B finetuned with KoVicuna dataset <https://huggingface.co/KRAFTON/KORani-v1-13B>
- 2ï¸âƒ£ KORani-v2-13B: LLaMA 13B finetuned with KoVicuna dataset  <https://huggingface.co/KRAFTON/KORani-v2-13B>
- 3ï¸âƒ£ KORani-v3-13B: LLaMA 13B finetuned with ShareGPT & KoVicuna datset <https://huggingface.co/KRAFTON/KORani-v3-13B>

### Notes
* We used LLaMA 13B from [here](https://huggingface.co/decapoda-research/llama-13b-hf).
* We extracted only the data from [Kovicuna](https://huggingface.co/datasets/junelee/sharegpt_deepl_ko) that corresponds to the first and second parts of the conversation, which are 'human' and 'GPT'.
* The model finetuning was conducted on eight A100 40GB GPUs. The code used for training is based on the [Fastchat](https://github.com/lm-sys/FastChat).

## Local Setup

1. Install dependencies
   ```bash
   pip install -r requirements.txt
   ```

## Inference
### Command <br>
    `--model_path` (str): model path for evaluation. (e.g. KRAFTON/KORani-v3-13B) <br>
    `--task` (str): choose which task you want to evaluate. (e.g. only [QA, summarization, translation] are available in this repo.) <br>


### Question Answering

    ```bash
    python inference.py \
        --model_path "KRAFTON/KORani-v3-13B"
        --task "QA"
    ```

    Below is the prompt used to generate the answer. You can modify it in the [QA link](prompts/QA.txt).

    ```python
    PROMPT = """\
    ìš°ë¦¬ëŠ” ì•„ë˜ì™€ ê°™ì€ ì •ë³´ë¥¼ ê°–ê³  ìˆìŠµë‹ˆë‹¤.
    ---------------------
    {context}
    ---------------------
    ### ì£¼ì–´ì§„ ì •ë³´ì— ë”°ë¼, ì§ˆë¬¸ì— ë‹µí•´ì£¼ì„¸ìš”.: '{question}'
    ### Assistant:"""
    ```


### Translation

    ```bash
    python inference.py \
        --model_path "KRAFTON/KORani-v3-13B"
        --task "translation"
    ```

    Below is the prompt used to generate the answer. You can modify it in the [translation link](prompts/translation.txt). If you use few-shot in translation, the performance can improve.

    ```python
    PROMPT = """\
    ### Instruction: Translate Korean sentence into English. You may leave specific names as they are.
    Korean: ì–¼ë§ˆë‚˜ ë§ì€ ì–¸ì–´ë¥¼ ë§í•  ìˆ˜ ìˆë‹ˆ?
    English: How many languages can you speak?#
    Korean: ì¼ ë‹¤ ëë‚¬ì–´?
    English: Did you finish your work?#
    Korean: {target_sentence} 
    English:"""
    ```


### Summarization

    ```bash
    python inference.py \
        --model_path "KRAFTON/KORani-v3-13B"
        --task "summarization"
    ```

    Below is the prompt used to generate the answer. You can modify it in the [summarization link](prompts/summarization.txt). It does not work for a max length of over 2048.

    ```python
    PROMPT = """\
    # Meeting note
    {target_document}

    # Summarize the meeting note into 3 Korean sentences.
    ### Output: 1)"""
    ```

## Evaluation
We tested model performance using GPT-4, and the code and results of the test can be found through the [AutoEvalGPT](https://github.com/krafton-ai/AutoEvalGPT).

## Limitations
The Korean performance of our models is not as good as the English performance of [Vicuna](https://github.com/lm-sys/FastChat). We believe this is due to the not enough quality of foundation models in the Korean tasks (compared to Llama in English tasks) and the dataset quality, which is primarily translational. We will continue to update the new versions of the Korani models as soon as we achieve better results.

## License
Our github repo and models are intended for research purpose, non-commercial use only, subject to the model License of LLaMA, [Terms of Use](https://openai.com/policies/terms-of-use) of the data generated by OpenAI, and [Privacy Practices](https://chrome.google.com/webstore/detail/sharegpt-share-your-chatg/daiacboceoaocpibfodeljbdfacokfjb) of ShareGPT. Please contact us If you find any potential violation.
The code is released under the Apache License 2.0.
