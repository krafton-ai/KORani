# AutoEvalGPT


## What is it?

Inspired by [Vicuna](https://vicuna.lmsys.org/) from lm-sys, we modified their prompts to be more robust and permutation invariable. 

## Prompt Format
```
{Task Description}
{Data}
{Answer 1}
{Answer 2}
We would like to request your feedback on the performance of two AI assistants in response to the user question displayed above.
Each assistant receives an overall score on a scale of 0 to 10, where a higher score indicates better overall performance. 
{Detailed metrics when evaluating the Task above}
Let's say the score of Assistant 1 is 3, 2, 2 for {metric 1}, {metric 2}, and {metric 3}, respectively.
Please first provide detailed explanations, specifying at which part of the example you think it is, avoiding any potential bias, and ensuring that the order in which the responses were presented does not affect your judgment. 
Please provide the detailed explanations specifying at which part of the example is better or worse and give the score to assistant 2 compared to the score of Assistant 1.  
At this time, evaluate the {metrics[k]} score only. You may speak in English.
```
You may adjust the score distribution of Assistant 1.

## What is different?
1. "Detailed Metrics" are generated by GPT-4 as well!
2. Evaluate one metric at a time. 

## Result
### Translation
